\chapter{Testing Techniques}\label{chap:testing}

Verification techniques, as discussed in Chapters~\ref{part:deep} and \ref{chap:drl}, are to ascertain -- with mathematical proof --  whether a property holds on a mathematical model. The soundness and completeness required by the mathematical proof result in the scalability problem that verification algorithms can only work with either small models (e.g., the MILP-based method as in Chapter~\ref{chap:MILP}) or limited number of input dimensions (e.g., the reachability analysis as in Chapter~\ref{chap:reachabilityAnalysis}). In practice, when working with real-world systems where the machine learning models are large in nature, other techniques have to be considered for the certification purpose. 
%
Similar to traditional software testing against software verification, neural network testing provides a certification methodology with a balance between completeness  and efficiency. In established industries, e.g., avionics and automotive, the needs for software testing have been settled in various standards such as DO-178C and MISRA. However, due to the lack of logical structures and system specification, it is less straightforward on how to extend such standards to work with systems with neural network components. In the following, we discuss some existing neural network testing techniques. The readers are referred to the survey~\cite{HUANG2020100270} for more discussion. 



\section{A General Testing Framework}\label{sec:testframework}

Assume that according to the model $f$ and a safety property $\phi$, we are able to define a set of test objectives $\requirements$. We will discuss later in Section~\ref{sec:test-criteria} a few existing covering methods $\coveringmethod$ to define $\requirements$ for convolutional neural networks. 

\begin{definition}[Test Suite]
Given a neural network $f$, a test suite $\testsuites$ is a finite set of input instances, i.e., $\testsuites\subseteq \mathcal{D}$. Each instance is called a test case. 
\end{definition}

%Usually, a test case is a single input $\testsuites\subseteq D_{1}$, e.g., in \cite{PCYJ2017}, or a pair of inputs $\testsuites\subseteq D_{1}\times D_{1}$, e.g., in \cite{sun2018testing-b}. 
Ideally, given the set of test objectives
$\requirements$ with respect to some covering method $\coveringmethod$, %$\requirements=(\coveringmethod,\testobjectives)$ 
we run a test case generation algorithm (to be introduced in Section~\ref{sec:testcasegen}) to find a test suite $\testsuites$ such that %
  \begin{equation}\label{equ:testframework}
  \forall \neuronpair\in \requirements \exists \textbf{x} \in \testsuites: \coveringmethod(\neuronpair,\textbf{x})
 \end{equation} 
where $\coveringmethod(\neuronpair,\textbf{x})$ intuitively means that the test objective $\neuronpair$ is satisfied under the test case $\textbf{x}$. Intuitively, Equation (\ref{equ:testframework}) means that every test objective is covered by some of the test cases. 
In practice, we might want to
compute the degree to which the test objectives are satisfied by a test suite $\testsuites$.


\begin{definition}[Test Criterion]
Given a neural network $f$, a covering method $\coveringmethod$, a set $\requirements$ of test objectives, % = (\coveringmethod,\testobjectives)$,
and a test suite $\testsuites$,  the test criterion $\metric_{\coveringmethod}(\requirements,\testsuites)$ is as follows: 
\begin{equation}
  \label{eq:madc}
  \metric_{\coveringmethod}(\requirements,\testsuites)=\frac{|\{\neuronpair \in \requirements | \exists  \textbf{x} \in \testsuites: \coveringmethod(\neuronpair,\textbf{x})\}|}{|\requirements|}
\end{equation}
\end{definition}
 
Intuitively, it computes the percentage of the test objectives that are
covered by test cases in $\testsuites$ w.r.t. the covering method~$\coveringmethod$. 


\section{Coverage Metrics for Neural Networks}
\label{sec:test-criteria}

Research in software engineering has resulted in a broad range of approaches
to testing software. Please refer to \cite{ZHM1997,JH2011,SWMPHS2017}  for comprehensive reviews.
In white-box testing, the structure of a
program is exploited to (perhaps automatically) generate test cases.
Structural coverage criteria (or metrics) define a set of test objectives to be covered, guiding the generation of test cases and evaluating the completeness of a test suite. 
E.g., a test suite with 100\% statement coverage exercises all statements of the program at
least once.  While it is arguable whether this ensures functional correctness,
high coverage is able to increase users' confidence (or trust) in the testing results~\cite{ZHM1997}.  Structural coverage analysis and testing are also
used as a means of assessment in a number of safety-critical scenarios, and
criteria such as statement and modified condition/decision coverage (MC/DC) are
applicable measures with respect to different criticality levels.  MC/DC was developed by NASA\cite{HVCR2001} and has been widely adopted.  It is used in avionics software development guidance to ensure adequate testing of applications with the highest criticality \cite{do178}.

%We let $\setofcoveringmethods$ be a set of covering methods, and
Let $\requirements$ be the set of test objectives to be covered. For different structure coverage, we can define different sets of test objectives. In \cite{PCYJ2017}, $\requirements$ is instantiated as the set of statuses of hidden neurons. That is, for the set $\mathcal{H}_i$ of hidden neurons with ReLU activation functions at layer $i$, we let 
\begin{equation}
    \requirements_{\text{neuron coverage}}=\bigcup_{i=2}^{K-1}\mathcal{H}_i
\end{equation}
Intuitively, combining with the general testing framework in Section~\ref{sec:testframework}, it requires to find a test suite $\testsuites$ such that for every hidden ReLU neuron, there is a test case $t\in \testsuites$ who can activate it.  
As another example, in \cite{sun2018testing-b},  $\requirements$ is instantiated as the set of causal relationships between feature pairs. That is, 
\begin{equation}
    \requirements_{\text{MC/DC coverage}}=\bigcup_{i=2}^{K-2}\{(h_1,h_2)~|~h_1\in \mathcal{H}_i, h_2\in \mathcal{H}_{i+1}\}
\end{equation}
Intuitively, combining with the general testing framework in Section~\ref{sec:testframework}, it requires to find a number of test pairs  such that  each $(h_1,h_2)\in \requirements_{\text{MC/DC coverage}}$, 
$h_2$ can be independently activated by $h_1$  \cite{DBLP:journals/corr/abs-1803-04792}.

%\begin{definition}[Test  Conditions] \label{def:coverageRequirement}
%Given a neural network $f$ and a  covering method $\coveringmethod\in \setofcoveringmethods$, a test objective set $\requirements$ is characterised by a pair $(\coveringmethod,\testobjectives)$ such that $\testobjectives\subseteq \neuronpairs(f)$.
%\end{definition}
 
%Intuitively, a test objective  $(\coveringmethod,\testobjectives)$ is to ask for the coverage of test objectives in $\testobjectives$ with the covering method $\coveringmethod$.  

\section{Test Case Generation}\label{sec:testcasegen}

Once a coverage metric is determined, it is needed to develop a test case generation algorithm to produce a set of test cases. Existing methods include e.g., input mutation~\cite{wicker2018feature}, fuzzing~\cite{odena2018tensorfuzz}, genetic algorithm~\cite{9451178}, symbolic execution~\cite{sun2018concolic}, and gradient ascent~\cite{sun2018concolicb}. 



\section{Discussion}

In addition to testing techniques which originate from the software engineering area, there are other techniques that might be useful to analyse the safety of neural networks. 
%
Statistical evaluation applies statistical methods in order to gain insights into the verification problem we concern. In addition to the purpose of determining the existence of failures in the deep learning model, statistical evaluation assesses the satisfiability of a property in a probabilistic way, by e.g.,
%obtaining assessment result via
aggregating sampling results. The aggregated evaluation result may have probabilistic guarantee, in the form of e.g., the probability of failure rate lower than a threshold $l$ is greater than $1-\epsilon$, for some small constant $\epsilon$. 



For the robustness, sampling methods, such as \cite{weng2018evaluating},
%,webb_statistical_2019}, 
are to summarise property-related statistics from the samples. 
%
While sampling methods can have probabilistic guarantees via e.g., Chebyshev's inequality, it is still under investigation on how to associate test coverage metrics with probabilistic guarantee. 
%
For the generalisation error, other than the empirical approach of using a set of test data to evaluate, recent efforts on complexity measure \cite{chatterji2019intriguing,jin2020does} suggest that it is possible to estimate generalisation error -- with theoretical bound -- by only considering the weights of the deep learning without resorting to the test dataset. 

